{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Core data handling and computation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Financial data downloading\n",
        "import yfinance as yf\n",
        "\n",
        "# PyTorch for our model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Date handling\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Type hints for better code clarity\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# Math operations\n",
        "import math"
      ],
      "metadata": {
        "id": "jhNVd4Rx6nov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2dXRcw36caA"
      },
      "outputs": [],
      "source": [
        "class HybridFinancialModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 state_dim,          # dimension of continuous state\n",
        "                 memory_dim,         # dimension of memory/attention state\n",
        "                 input_dim,          # dimension of market inputs\n",
        "                 num_heads=8,        # number of attention heads\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # State space components\n",
        "        self.state_dynamics = StateEvolutionModule(\n",
        "            state_dim=state_dim,\n",
        "            memory_dim=memory_dim,\n",
        "            input_dim=input_dim\n",
        "        )\n",
        "\n",
        "        # Attention components\n",
        "        self.attention = MultiHeadAttention(\n",
        "            state_dim=state_dim,\n",
        "            memory_dim=memory_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Coupling layers\n",
        "        self.state_to_memory = StateProjModule(state_dim, memory_dim)\n",
        "        self.memory_to_dynamics = MemoryProjModule(memory_dim, state_dim)\n",
        "\n",
        "        # Output layers\n",
        "        self.prediction_head = PredictionModule(state_dim, memory_dim)\n",
        "\n",
        "    def forward(self, x, h=None, m=None, dt=0.1):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Initialize states if not provided\n",
        "        if h is None:\n",
        "            h = torch.zeros(batch_size, self.state_dim).to(x.device)\n",
        "        if m is None:\n",
        "            m = torch.zeros(batch_size, self.memory_dim).to(x.device)\n",
        "\n",
        "        # Project current state to memory space\n",
        "        memory_projection = self.state_to_memory(h)\n",
        "\n",
        "        # Update memory through attention\n",
        "        past_states = h  # In practice, you might want to maintain a longer history\n",
        "        m_new = self.attention(h, past_states)\n",
        "\n",
        "        # Project memory to state space\n",
        "        state_influence = self.memory_to_dynamics(m_new, h)\n",
        "\n",
        "        # Update state\n",
        "        h_new = self.state_dynamics(h, state_influence, x, dt)\n",
        "\n",
        "        # Generate predictions\n",
        "        predictions = self.prediction_head(h_new, m_new)\n",
        "\n",
        "        return predictions, h_new, m_new\n",
        "\n",
        "class StateEvolutionModule(nn.Module):\n",
        "    def __init__(self, state_dim, memory_dim, input_dim):\n",
        "        super().__init__()\n",
        "        # A(m) matrix - state transition conditioned on memory\n",
        "        self.dynamics_net = nn.Sequential(\n",
        "            nn.Linear(memory_dim, state_dim * state_dim),\n",
        "            nn.Tanh()  # Ensure stability of dynamics\n",
        "        )\n",
        "        # B matrix - input effect\n",
        "        self.input_proj = nn.Linear(input_dim, state_dim)\n",
        "\n",
        "    def forward(self, h, m, x, dt):\n",
        "        # Compute state transition matrix based on current memory\n",
        "        A = self.dynamics_net(m).view(-1, self.state_dim, self.state_dim)\n",
        "\n",
        "        # Continuous state update using Euler integration\n",
        "        dh = torch.bmm(A, h.unsqueeze(-1)).squeeze(-1) + self.input_proj(x)\n",
        "        h_next = h + dh * dt\n",
        "\n",
        "        return h_next\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, state_dim, memory_dim, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        # Standard multi-head attention with additional conditioning\n",
        "        self.q_proj = nn.Linear(state_dim, memory_dim)\n",
        "        self.k_proj = nn.Linear(state_dim, memory_dim)\n",
        "        self.v_proj = nn.Linear(state_dim, memory_dim)\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = memory_dim // num_heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, h, past_states):\n",
        "        # Project current state to Q/K/V space\n",
        "        Q = self.q_proj(h).view(-1, self.num_heads, self.head_dim)\n",
        "        K = self.k_proj(past_states).view(-1, self.num_heads, self.head_dim)\n",
        "        V = self.v_proj(past_states).view(-1, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.bmm(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Compute memory update\n",
        "        m = torch.bmm(attn, V)\n",
        "        return m.view(-1, self.num_heads * self.head_dim)\n",
        "\n",
        "class StateProjModule(nn.Module):\n",
        "    def __init__(self, state_dim, memory_dim):\n",
        "        super().__init__()\n",
        "        # We create a more sophisticated projection than just a linear layer\n",
        "        # to allow the state to influence memory in a flexible way\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(state_dim, state_dim * 2),\n",
        "            nn.LayerNorm(state_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(state_dim * 2, memory_dim),\n",
        "            nn.LayerNorm(memory_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Project the continuous state into the memory space\n",
        "        # This allows the memory system to \"understand\" the current dynamics\n",
        "        memory_projection = self.projection(state)\n",
        "        return memory_projection\n",
        "\n",
        "class MemoryProjModule(nn.Module):\n",
        "    def __init__(self, memory_dim, state_dim):\n",
        "        super().__init__()\n",
        "        # Similar to StateProjModule, but going the other direction\n",
        "        # We want the memory to be able to influence the state dynamics\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(memory_dim, memory_dim * 2),\n",
        "            nn.LayerNorm(memory_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(memory_dim * 2, state_dim),\n",
        "            nn.LayerNorm(state_dim)\n",
        "        )\n",
        "\n",
        "        # Additional component to help regulate the influence of memory\n",
        "        # This acts as a \"gating\" mechanism\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(memory_dim + state_dim, state_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, memory, current_state):\n",
        "        # Project memory into state space\n",
        "        state_influence = self.projection(memory)\n",
        "\n",
        "        # Compute a gate that determines how much the memory\n",
        "        # should influence the state\n",
        "        gate_input = torch.cat([memory, current_state], dim=-1)\n",
        "        influence_gate = self.gate(gate_input)\n",
        "\n",
        "        # Apply the gated influence\n",
        "        gated_influence = state_influence * influence_gate\n",
        "        return gated_influence\n",
        "\n",
        "class PredictionModule(nn.Module):\n",
        "    def __init__(self, state_dim, memory_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Combined dimension of state and memory information\n",
        "        combined_dim = state_dim + memory_dim\n",
        "\n",
        "        # Main prediction network\n",
        "        self.prediction_network = nn.Sequential(\n",
        "            nn.Linear(combined_dim, combined_dim * 2),\n",
        "            nn.LayerNorm(combined_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(combined_dim * 2, combined_dim),\n",
        "            nn.LayerNorm(combined_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Separate heads for different prediction tasks\n",
        "        self.direction_head = nn.Linear(combined_dim, 1)  # Predicts price movement direction\n",
        "        self.magnitude_head = nn.Linear(combined_dim, 1)  # Predicts size of movement\n",
        "        self.uncertainty_head = nn.Linear(combined_dim, 1)  # Predicts prediction uncertainty\n",
        "\n",
        "    def forward(self, state, memory):\n",
        "        # Combine state and memory information\n",
        "        combined_features = torch.cat([state, memory], dim=-1)\n",
        "\n",
        "        # Process through main network\n",
        "        processed_features = self.prediction_network(combined_features)\n",
        "\n",
        "        # Generate predictions\n",
        "        predictions = {\n",
        "            'direction': self.direction_head(processed_features),  # Up/down prediction\n",
        "            'magnitude': self.magnitude_head(processed_features),  # Expected size of move\n",
        "            'uncertainty': torch.exp(self.uncertainty_head(processed_features))  # Uncertainty estimate\n",
        "        }\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal Market Dynamics\n",
        "\n",
        "The first benchmark tests the model's ability to handle standard market behavior. We'll use the S&P 500 and its constituent stocks, focusing on daily data over the past 20 years"
      ],
      "metadata": {
        "id": "oY7yVpwf8okx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarketDynamicsBenchmark:\n",
        "    def __init__(self, start_date: datetime = \"2004-01-01\",\n",
        "                 end_date: datetime = \"2024-01-01\"):\n",
        "        # Load market data\n",
        "        self.market_data = self._load_market_data()\n",
        "\n",
        "        # Features we'll track\n",
        "        self.features = [\n",
        "            'close', 'volume', 'vwap', 'volatility_10d',\n",
        "            'rsi_14', 'macd', 'bb_upper', 'bb_lower'\n",
        "        ]\n",
        "\n",
        "        # Performance metrics\n",
        "        self.metrics = {\n",
        "            'sharpe_ratio': self._calculate_sharpe,\n",
        "            'sortino_ratio': self._calculate_sortino,\n",
        "            'max_drawdown': self._calculate_drawdown,\n",
        "            'hit_ratio': self._calculate_hit_ratio\n",
        "        }\n",
        "\n",
        "    def evaluate_model(self, model, test_period: Tuple[datetime, datetime]) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluates model performance during normal market conditions\n",
        "        Returns dictionary of performance metrics\n",
        "        \"\"\"\n",
        "        # Implementation details here"
      ],
      "metadata": {
        "id": "LM2iKuWe8jF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegimeChangeBenchmark:\n",
        "    def __init__(self):\n",
        "        # Define known regime change periods\n",
        "        self.regime_changes = {\n",
        "            '2008-09-15': 'Lehman Brothers Bankruptcy',\n",
        "            '2020-03-23': 'COVID-19 Market Bottom',\n",
        "            '2022-01-03': 'Fed Tightening Cycle'\n",
        "        }\n",
        "\n",
        "        # Metrics specific to regime changes\n",
        "        self.transition_metrics = {\n",
        "            'detection_lag': self._calculate_detection_lag,\n",
        "            'false_positives': self._calculate_false_positives,\n",
        "            'adaptation_speed': self._calculate_adaptation_speed\n",
        "        }\n",
        "\n",
        "    def evaluate_regime_handling(self, model) -> Dict:\n",
        "        \"\"\"\n",
        "        Tests model's ability to:\n",
        "        1. Detect regime changes\n",
        "        2. Adapt to new market conditions\n",
        "        3. Maintain performance during transitions\n",
        "        \"\"\"\n",
        "        # Implementation details here"
      ],
      "metadata": {
        "id": "cjBkBANx8sPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EventResponseBenchmark:\n",
        "    def __init__(self):\n",
        "        # Define different types of events to test\n",
        "        self.event_types = {\n",
        "            'earnings': self._load_earnings_events(),\n",
        "            'fed_announcements': self._load_fed_events(),\n",
        "            'economic_data': self._load_economic_events()\n",
        "        }\n",
        "\n",
        "        # Event-specific metrics\n",
        "        self.event_metrics = {\n",
        "            'prediction_accuracy': self._calculate_event_accuracy,\n",
        "            'reaction_time': self._calculate_reaction_time,\n",
        "            'profit_per_event': self._calculate_event_profit\n",
        "        }\n",
        "\n",
        "    def evaluate_event_handling(self, model, event_type: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Tests model's performance around specific event types\n",
        "        \"\"\"\n",
        "        # Implementation details here"
      ],
      "metadata": {
        "id": "J8Yu2i-B8u44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModels:\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'lstm': self._create_lstm_baseline(),\n",
        "            'transformer': self._create_transformer_baseline(),\n",
        "            'state_space': self._create_ssm_baseline(),\n",
        "            'random_forest': self._create_rf_baseline()\n",
        "        }"
      ],
      "metadata": {
        "id": "mhQM2zZ68yJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PracticalMetrics:\n",
        "    def __init__(self):\n",
        "        self.trading_metrics = {\n",
        "            'latency': self._measure_inference_latency,\n",
        "            'memory_usage': self._measure_memory_usage,\n",
        "            'trade_execution_lag': self._measure_execution_lag,\n",
        "            'transaction_costs': self._calculate_transaction_costs\n",
        "        }\n",
        "\n",
        "        self.risk_metrics = {\n",
        "            'var_95': self._calculate_var,\n",
        "            'expected_shortfall': self._calculate_es,\n",
        "            'beta': self._calculate_beta,\n",
        "            'correlation_stability': self._measure_correlation_stability\n",
        "        }"
      ],
      "metadata": {
        "id": "QT5XIc_V8z5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MarketDataHandler:\n",
        "    def __init__(self,\n",
        "                 symbols: List[str],\n",
        "                 lookback_period: int = 252):  # One trading year\n",
        "        self.symbols = symbols\n",
        "        self.lookback_period = lookback_period\n",
        "\n",
        "        # Create data buffers for model features\n",
        "        self.price_history = {}\n",
        "        self.feature_buffers = {}\n",
        "\n",
        "    def update_market_data(self, new_data: Dict):\n",
        "        \"\"\"\n",
        "        Updates internal buffers with new market data\n",
        "        Ensures we maintain proper history for feature calculation\n",
        "        \"\"\"\n",
        "        for symbol in self.symbols:\n",
        "            # Update price and volume data\n",
        "            self.price_history[symbol] = self.price_history[symbol].append(\n",
        "                new_data[symbol]\n",
        "            ).tail(self.lookback_period)\n",
        "\n",
        "            # Recalculate features\n",
        "            self._update_features(symbol)\n",
        "\n",
        "    def _update_features(self, symbol: str):\n",
        "        \"\"\"\n",
        "        Calculates all necessary features for the model\n",
        "        \"\"\"\n",
        "        data = self.price_history[symbol]\n",
        "\n",
        "        # Calculate technical indicators\n",
        "        self.feature_buffers[symbol] = {\n",
        "            'returns': self._calculate_returns(data),\n",
        "            'volatility': self._calculate_volatility(data),\n",
        "            'volume_ma': self._calculate_volume_ma(data),\n",
        "            # Add more features as needed\n",
        "        }"
      ],
      "metadata": {
        "id": "o8Un9Im5AekO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingModel:\n",
        "    def __init__(self,\n",
        "                 model: HybridFinancialModel,\n",
        "                 risk_limits: Dict[str, float]):\n",
        "        self.model = model\n",
        "        self.risk_limits = risk_limits\n",
        "        self.current_state = None\n",
        "        self.current_memory = None\n",
        "\n",
        "    def predict(self,\n",
        "                market_data: MarketDataHandler,\n",
        "                current_positions: Dict[str, float]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Generates trading signals while respecting risk limits\n",
        "        Returns: Dictionary of recommended position sizes\n",
        "        \"\"\"\n",
        "        # Prepare features for model input\n",
        "        features = self._prepare_features(market_data)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            self.current_state, self.current_memory = self.model(\n",
        "                features,\n",
        "                self.current_state,\n",
        "                self.current_memory\n",
        "            )\n",
        "\n",
        "        # Convert predictions to position sizes\n",
        "        positions = self._predictions_to_positions(\n",
        "            self.current_state,\n",
        "            current_positions,\n",
        "            self.risk_limits\n",
        "        )\n",
        "\n",
        "        return positions"
      ],
      "metadata": {
        "id": "nDKurvcaAfKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RiskManager:\n",
        "    def __init__(self,\n",
        "                 max_position_size: float,\n",
        "                 max_portfolio_var: float,\n",
        "                 max_leverage: float):\n",
        "        self.max_position_size = max_position_size\n",
        "        self.max_portfolio_var = max_portfolio_var\n",
        "        self.max_leverage = max_leverage\n",
        "\n",
        "    def validate_trades(self,\n",
        "                       proposed_trades: Dict[str, float],\n",
        "                       current_positions: Dict[str, float],\n",
        "                       market_data: MarketDataHandler) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Adjusts proposed trades to ensure they meet risk limits\n",
        "        Returns: Dictionary of approved position changes\n",
        "        \"\"\"\n",
        "        # Check position size limits\n",
        "        adjusted_trades = self._check_position_limits(proposed_trades)\n",
        "\n",
        "        # Check portfolio VaR\n",
        "        adjusted_trades = self._check_portfolio_risk(\n",
        "            adjusted_trades,\n",
        "            current_positions,\n",
        "            market_data\n",
        "        )\n",
        "\n",
        "        # Check leverage constraints\n",
        "        final_trades = self._check_leverage(\n",
        "            adjusted_trades,\n",
        "            current_positions\n",
        "        )\n",
        "\n",
        "        return final_trades"
      ],
      "metadata": {
        "id": "YyKDn0ZrAi-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExecutionEngine:\n",
        "    def __init__(self,\n",
        "                 broker_api,\n",
        "                 slippage_model: Optional[callable] = None):\n",
        "        self.broker_api = broker_api\n",
        "        self.slippage_model = slippage_model or self._default_slippage\n",
        "        self.pending_orders = {}\n",
        "\n",
        "    async def execute_trades(self,\n",
        "                           trades: Dict[str, float],\n",
        "                           market_data: MarketDataHandler):\n",
        "        \"\"\"\n",
        "        Executes trades while managing transaction costs and market impact\n",
        "        \"\"\"\n",
        "        for symbol, target_size in trades.items():\n",
        "            # Calculate optimal trade size considering market impact\n",
        "            trade_size = self._calculate_optimal_size(\n",
        "                symbol,\n",
        "                target_size,\n",
        "                market_data\n",
        "            )\n",
        "\n",
        "            # Estimate transaction costs\n",
        "            costs = self._estimate_transaction_costs(\n",
        "                symbol,\n",
        "                trade_size,\n",
        "                market_data\n",
        "            )\n",
        "\n",
        "            if self._is_trade_profitable(trade_size, costs):\n",
        "                await self._submit_order(symbol, trade_size)"
      ],
      "metadata": {
        "id": "S9prameiAk9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PaperTradingEnvironment:\n",
        "    def __init__(self,\n",
        "                 initial_capital: float = 1_000_000,  # Start with $1M pseudo-capital\n",
        "                 data_source: str = 'yahoo',  # Or your preferred data source\n",
        "                 transaction_costs: bool = True,\n",
        "                 market_impact: bool = True):\n",
        "\n",
        "        self.capital = initial_capital\n",
        "        self.positions = {}\n",
        "        self.trade_history = []\n",
        "\n",
        "        # Critical: Implement realistic market friction\n",
        "        self.transaction_costs = {\n",
        "            'commission': 0.001,  # 0.1% commission\n",
        "            'slippage': 0.001,    # 0.1% assumed slippage\n",
        "            'market_impact': self._calculate_market_impact if market_impact else None\n",
        "        }\n",
        "\n",
        "        # Important: Track all possible failure modes\n",
        "        self.system_checks = {\n",
        "            'data_delays': self._simulate_data_delays,\n",
        "            'execution_fails': self._simulate_execution_failures,\n",
        "            'price_gaps': self._simulate_price_gaps\n",
        "        }\n",
        "\n",
        "    def execute_trade(self, symbol: str, size: float, current_price: float):\n",
        "        \"\"\"\n",
        "        Simulates trade execution with realistic constraints\n",
        "        \"\"\"\n",
        "        # Simulate potential execution delay\n",
        "        execution_delay = self._simulate_latency()\n",
        "\n",
        "        # Get realistic fill price including market impact\n",
        "        fill_price = self._calculate_fill_price(\n",
        "            symbol, size, current_price, execution_delay\n",
        "        )\n",
        "\n",
        "        # Calculate total transaction costs\n",
        "        costs = self._calculate_total_costs(size, fill_price)\n",
        "\n",
        "        # Update capital and positions\n",
        "        if self._can_execute_trade(size * fill_price + costs):\n",
        "            self.capital -= (size * fill_price + costs)\n",
        "            self.positions[symbol] = self.positions.get(symbol, 0) + size\n",
        "\n",
        "            # Record trade details for analysis\n",
        "            self._record_trade(symbol, size, fill_price, costs)\n",
        "\n",
        "    def _calculate_fill_price(self, symbol: str, size: float,\n",
        "                            current_price: float, delay: float) -> float:\n",
        "        \"\"\"\n",
        "        Calculates realistic fill price including:\n",
        "        - Price movement during execution delay\n",
        "        - Market impact based on order size vs. average volume\n",
        "        - Bid-ask spread\n",
        "        \"\"\"\n",
        "        # Implementation details here"
      ],
      "metadata": {
        "id": "t1CIG6U2ALyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stress_test_strategy(strategy, environment):\n",
        "    # Test during normal markets\n",
        "    normal_market_results = run_test_period(\n",
        "        strategy, environment,\n",
        "        start_date='2019-01-01',\n",
        "        end_date='2019-12-31'\n",
        "    )\n",
        "\n",
        "    # Test during high volatility\n",
        "    crisis_results = run_test_period(\n",
        "        strategy, environment,\n",
        "        start_date='2020-03-01',\n",
        "        end_date='2020-04-30'\n",
        "    )\n",
        "\n",
        "    # Test during low liquidity\n",
        "    holiday_results = run_test_period(\n",
        "        strategy, environment,\n",
        "        start_date='2019-12-24',\n",
        "        end_date='2019-12-26'\n",
        "    )"
      ],
      "metadata": {
        "id": "8dZqoIYVASUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StrategyMonitor:\n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            'sharpe_ratio': self._calculate_sharpe,\n",
        "            'max_drawdown': self._calculate_drawdown,\n",
        "            'win_rate': self._calculate_win_rate,\n",
        "            'profit_factor': self._calculate_profit_factor,\n",
        "            'recovery_time': self._calculate_recovery_time\n",
        "        }\n",
        "\n",
        "        # Critical: Track strategy degradation\n",
        "        self.degradation_metrics = {\n",
        "            'signal_decay': self._measure_signal_decay,\n",
        "            'increased_correlation': self._measure_correlation_changes,\n",
        "            'reduced_capacity': self._measure_capacity_constraints\n",
        "        }"
      ],
      "metadata": {
        "id": "v6GgOd-oASwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reality_check(strategy_results: Dict) -> bool:\n",
        "    \"\"\"\n",
        "    Returns False if strategy shows suspicious performance\n",
        "    \"\"\"\n",
        "    suspicious_patterns = [\n",
        "        _check_unrealistic_sharpe(strategy_results),\n",
        "        _check_perfect_timing(strategy_results),\n",
        "        _check_impossible_fills(strategy_results),\n",
        "        _check_capacity_constraints(strategy_results)\n",
        "    ]\n",
        "\n",
        "    return not any(suspicious_patterns)"
      ],
      "metadata": {
        "id": "b_AT8GOAAZDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self,\n",
        "                 model: HybridFinancialModel,\n",
        "                 training_start: str = '2010-01-01',\n",
        "                 training_end: str = '2022-12-31',\n",
        "                 batch_size: int = 32,\n",
        "                 sequence_length: int = 252):  # One trading year\n",
        "\n",
        "        self.model = model\n",
        "        self.batch_size = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # Initialize data loaders\n",
        "        self.data_handler = FinancialDataHandler(\n",
        "            training_start=training_start,\n",
        "            training_end=training_end\n",
        "        )\n",
        "\n",
        "\n",
        "    def create_data_loader(self, features: torch.Tensor, labels: torch.Tensor) -> torch.utils.data.DataLoader:\n",
        "        \"\"\"\n",
        "        Creates a DataLoader that efficiently feeds data to our model during training.\n",
        "\n",
        "        The DataLoader handles:\n",
        "        1. Batching the data into consistent sizes\n",
        "        2. Shuffling the data (while maintaining sequence order within each sample)\n",
        "        3. Loading data efficiently in parallel\n",
        "\n",
        "        Args:\n",
        "            features: Tensor of shape (num_samples, sequence_length, num_features)\n",
        "            labels: Tensor of shape (num_samples, num_prediction_targets)\n",
        "\n",
        "        Returns:\n",
        "            DataLoader that yields batches of (features, labels)\n",
        "        \"\"\"\n",
        "        # Create a TensorDataset that pairs features with labels\n",
        "        dataset = torch.utils.data.TensorDataset(features, labels)\n",
        "\n",
        "        # Create the DataLoader with appropriate settings\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,  # Shuffle data between epochs\n",
        "            num_workers=2,  # Use 2 worker processes for loading data\n",
        "            pin_memory=True,  # Speed up data transfer to GPU if used\n",
        "            drop_last=True  # Drop the last incomplete batch if any\n",
        "        )\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    def prepare_training_data(self):\n",
        "        \"\"\"\n",
        "        Prepares and stores training data, splitting it into training and validation sets.\n",
        "        This method processes our raw financial data into a format suitable for training\n",
        "        our hybrid model.\n",
        "        \"\"\"\n",
        "        # Get features and labels from our data handler\n",
        "        features, labels = self.data_handler.prepare_training_data(self.sequence_length)\n",
        "\n",
        "        # Split into training and validation sets (80-20 split)\n",
        "        # We use a time-based split since this is financial data\n",
        "        split_idx = int(0.8 * len(features))\n",
        "\n",
        "        # Store the split data as class attributes\n",
        "        self.train_data = {\n",
        "            'features': features[:split_idx],\n",
        "            'labels': labels[:split_idx]\n",
        "        }\n",
        "\n",
        "        self.val_data = {\n",
        "            'features': features[split_idx:],\n",
        "            'labels': labels[split_idx:]\n",
        "        }\n",
        "\n",
        "        print(f\"Training data shape: {self.train_data['features'].shape}\")\n",
        "        print(f\"Validation data shape: {self.val_data['features'].shape}\")\n",
        "\n",
        "\n",
        "\n",
        "    def train_model(self, epochs: int = 100, learning_rate: float = 1e-4):\n",
        "        \"\"\"\n",
        "        Trains the hybrid model using our prepared training and validation data.\n",
        "        \"\"\"\n",
        "        # Create data loaders using our stored data\n",
        "        train_loader = self.create_data_loader(\n",
        "            self.train_data['features'],\n",
        "            self.train_data['labels']\n",
        "        )\n",
        "\n",
        "        val_loader = self.create_data_loader(\n",
        "            self.val_data['features'],\n",
        "            self.val_data['labels']\n",
        "        )\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training loop\n",
        "            self.model.train()\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                state_pred, memory = self.model(batch['features'])\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = loss_fn(\n",
        "                    state_pred=state_pred,\n",
        "                    memory=memory,\n",
        "                    targets=batch['labels']\n",
        "                )\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_metrics = self.validate_epoch(val_loader)\n",
        "\n",
        "            # Early stopping check\n",
        "            if self.should_stop_early(val_metrics):\n",
        "                break\n",
        "\n",
        "    def create_loss_function(self):\n",
        "        \"\"\"\n",
        "        Creates a composite loss function appropriate for financial data\n",
        "        \"\"\"\n",
        "        def custom_loss(state_pred, memory, targets):\n",
        "            # Directional accuracy loss\n",
        "            direction_loss = F.binary_cross_entropy_with_logits(\n",
        "                state_pred['direction'],\n",
        "                targets['direction']\n",
        "            )\n",
        "\n",
        "            # Magnitude loss with asymmetric penalties\n",
        "            magnitude_loss = self.asymmetric_magnitude_loss(\n",
        "                state_pred['magnitude'],\n",
        "                targets['magnitude']\n",
        "            )\n",
        "\n",
        "            # Temporal coherence loss for state evolution\n",
        "            coherence_loss = self.temporal_coherence_loss(state_pred)\n",
        "\n",
        "            return (\n",
        "                0.4 * direction_loss +\n",
        "                0.4 * magnitude_loss +\n",
        "                0.2 * coherence_loss\n",
        "            )\n",
        "\n",
        "        return custom_loss"
      ],
      "metadata": {
        "id": "acGkFhEABvnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FinancialDataHandler:\n",
        "    def __init__(self,\n",
        "                 training_start: str,\n",
        "                 training_end: str,\n",
        "                 symbols: List[str] = ['SPY'],  # Default to S&P 500 ETF\n",
        "                 data_source: str = 'yahoo'):\n",
        "\n",
        "        self.training_start = training_start\n",
        "        self.training_end = training_end\n",
        "        self.symbols = symbols\n",
        "        self.data_source = data_source\n",
        "\n",
        "        # Initialize feature calculators\n",
        "        self.technical_features = [\n",
        "            self._calculate_returns,\n",
        "            self._calculate_volatility,\n",
        "            self._calculate_volume_features,\n",
        "            self._calculate_price_patterns\n",
        "        ]\n",
        "\n",
        "        self.market_features = [\n",
        "            self._calculate_market_regime,\n",
        "            self._calculate_correlation_structure,\n",
        "            self._calculate_liquidity_metrics\n",
        "        ]\n",
        "\n",
        "        # Load and prepare initial data\n",
        "        self.data = self._load_market_data()\n",
        "\n",
        "    def _load_market_data(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Loads market data for the specified period and symbols\n",
        "        \"\"\"\n",
        "        data = {}\n",
        "        for symbol in self.symbols:\n",
        "            # Using yfinance to load data\n",
        "            data[symbol] = yf.download(\n",
        "                symbol,\n",
        "                start=self.training_start,\n",
        "                end=self.training_end\n",
        "            )\n",
        "\n",
        "        # Combine all symbols into one DataFrame\n",
        "        combined_data = pd.concat(data.values(), axis=1, keys=data.keys())\n",
        "        return combined_data\n",
        "\n",
        "    def _calculate_returns(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculates various return-based features\n",
        "        \"\"\"\n",
        "        features = pd.DataFrame(index=data.index)\n",
        "\n",
        "        # Daily returns\n",
        "        features['returns'] = data['Close'].pct_change()\n",
        "\n",
        "        # Rolling returns for different periods\n",
        "        for window in [5, 10, 21]:  # 1 week, 2 weeks, 1 month\n",
        "            features[f'returns_{window}d'] = data['Close'].pct_change(window)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_volatility(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculates volatility-based features\n",
        "        \"\"\"\n",
        "        features = pd.DataFrame(index=data.index)\n",
        "\n",
        "        # Rolling volatility for different periods\n",
        "        for window in [10, 21, 63]:  # 2 weeks, 1 month, 3 months\n",
        "            returns = data['Close'].pct_change()\n",
        "            features[f'volatility_{window}d'] = returns.rolling(window).std()\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_volume_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculates volume-based features\n",
        "        \"\"\"\n",
        "        features = pd.DataFrame(index=data.index)\n",
        "\n",
        "        # Volume changes\n",
        "        features['volume_change'] = data['Volume'].pct_change()\n",
        "\n",
        "        # Rolling average volume\n",
        "        for window in [5, 10, 21]:\n",
        "            features[f'volume_ma_{window}d'] = data['Volume'].rolling(window).mean()\n",
        "\n",
        "        # Volume relative to moving average\n",
        "        features['volume_rel_ma'] = data['Volume'] / features['volume_ma_21d']\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_price_patterns(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculates technical price patterns and indicators\n",
        "        Returns DataFrame with price-based technical indicators\n",
        "        \"\"\"\n",
        "        features = pd.DataFrame(index=data.index)\n",
        "\n",
        "        # Moving averages for different timeframes\n",
        "        for window in [5, 10, 20, 50]:\n",
        "            features[f'sma_{window}d'] = data['Close'].rolling(window=window).mean()\n",
        "\n",
        "        # Price relative to moving averages (momentum indicators)\n",
        "        features['price_sma_ratio_20d'] = data['Close'] / features['sma_20d']\n",
        "\n",
        "        # RSI (Relative Strength Index)\n",
        "        delta = data['Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        features['rsi_14d'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # Bollinger Bands\n",
        "        sma_20 = data['Close'].rolling(window=20).mean()\n",
        "        std_20 = data['Close'].rolling(window=20).std()\n",
        "        features['bb_upper'] = sma_20 + (std_20 * 2)\n",
        "        features['bb_lower'] = sma_20 - (std_20 * 2)\n",
        "        features['bb_position'] = (data['Close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])\n",
        "\n",
        "        # MACD (Moving Average Convergence Divergence)\n",
        "        exp1 = data['Close'].ewm(span=12, adjust=False).mean()\n",
        "        exp2 = data['Close'].ewm(span=26, adjust=False).mean()\n",
        "        features['macd'] = exp1 - exp2\n",
        "        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "        # Handle any NaN values created by the rolling windows\n",
        "        features = features.fillna(method='bfill')\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_market_regime(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Identifies market regimes using various metrics.\n",
        "        Market regimes help us understand the broader context of market behavior,\n",
        "        such as whether we're in a bull market, bear market, or high volatility period.\n",
        "        \"\"\"\n",
        "        features = pd.DataFrame(index=data.index)\n",
        "\n",
        "        # First, let's calculate some trend indicators\n",
        "        # We'll use multiple timeframes to capture different regime aspects\n",
        "        returns = data['Close'].pct_change()\n",
        "\n",
        "        # Trend strength using moving averages\n",
        "        sma_50 = data['Close'].rolling(window=50).mean()\n",
        "        sma_200 = data['Close'].rolling(window=200).mean()\n",
        "\n",
        "        # Golden/Death Cross indicator\n",
        "        features['trend_regime'] = (sma_50 > sma_200).astype(float)\n",
        "\n",
        "        # Volatility regime using realized volatility\n",
        "        # We compare short-term vs long-term volatility\n",
        "        vol_20 = returns.rolling(window=20).std()\n",
        "        vol_60 = returns.rolling(window=60).std()\n",
        "        features['volatility_regime'] = (vol_20 > vol_60).astype(float)\n",
        "\n",
        "        # Market momentum regime\n",
        "        # Using 3-month and 12-month returns\n",
        "        features['momentum_regime'] = (data['Close'].pct_change(63) > 0).astype(float)\n",
        "\n",
        "        # Composite regime indicator combining all aspects\n",
        "        features['composite_regime'] = (\n",
        "            features['trend_regime'] +\n",
        "            features['volatility_regime'] +\n",
        "            features['momentum_regime']\n",
        "        ) / 3.0\n",
        "\n",
        "        # Handle any NaN values from the rolling calculations\n",
        "        features = features.fillna(method='bfill')\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_correlation_structure(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Analyzes the correlation structure of the market. This helps us understand\n",
        "        how different market components are interacting with each other, which\n",
        "        can signal important regime changes or risk conditions.\n",
        "        \"\"\"\n",
        "        features = pd.DataFrame(index=data.index)\n",
        "\n",
        "        # Calculate returns for our correlation analysis\n",
        "        returns = data['Close'].pct_change()\n",
        "\n",
        "        # Rolling correlation with market (if we have multiple symbols)\n",
        "        if len(self.symbols) > 1:\n",
        "            market_returns = returns[self.symbols[0]]  # Assuming first symbol is market\n",
        "            for symbol in self.symbols[1:]:\n",
        "                # 20-day rolling correlation with market\n",
        "                features[f'market_corr_{symbol}'] = returns[symbol].rolling(20).corr(market_returns)\n",
        "\n",
        "        # Correlation stability - how stable are correlations over time?\n",
        "        # We calculate this by looking at the standard deviation of correlations\n",
        "        window_sizes = [20, 60]  # Look at both month and quarter timeframes\n",
        "\n",
        "        for window in window_sizes:\n",
        "            # Calculate rolling mean of absolute returns (a measure of volatility)\n",
        "            vol = returns.abs().rolling(window=window).mean()\n",
        "\n",
        "            # Calculate how this volatility measure correlates with returns\n",
        "            # This helps identify if we're in a risk-on/risk-off regime\n",
        "            features[f'vol_ret_corr_{window}d'] = (\n",
        "                returns.rolling(window=window)\n",
        "                .corr(vol)\n",
        "            )\n",
        "\n",
        "            # Dispersion of returns - helps identify if market is moving as one\n",
        "            if len(self.symbols) > 1:\n",
        "                features[f'return_dispersion_{window}d'] = (\n",
        "                    returns.std(axis=1)\n",
        "                    .rolling(window=window)\n",
        "                    .mean()\n",
        "                )\n",
        "\n",
        "        # Risk-on/Risk-off indicator using correlation between\n",
        "        # high volatility and high correlation periods\n",
        "        features['risk_regime'] = (\n",
        "            features['vol_ret_corr_20d']\n",
        "            .rolling(window=10)\n",
        "            .mean()\n",
        "        )\n",
        "\n",
        "        # Handle any NaN values from the rolling calculations\n",
        "        features = features.fillna(method='bfill')\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_liquidity_metrics(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculates various liquidity metrics to understand how easily we can trade\n",
        "        without significantly impacting price. Liquidity is crucial for understanding\n",
        "        transaction costs and execution risk.\n",
        "        \"\"\"\n",
        "        features = pd.DataFrame(index=data.index)\n",
        "\n",
        "        # First, let's calculate basic volume-based liquidity measures\n",
        "        typical_price = (data['High'] + data['Low'] + data['Close']) / 3\n",
        "\n",
        "        # Dollar volume - a fundamental measure of trading activity\n",
        "        features['dollar_volume'] = typical_price * data['Volume']\n",
        "\n",
        "        # Amihud illiquidity ratio\n",
        "        # This measures price impact per dollar of trading volume\n",
        "        # Higher values indicate lower liquidity\n",
        "        daily_returns = data['Close'].pct_change().abs()\n",
        "        features['amihud_ratio'] = (daily_returns / features['dollar_volume']) * 1e6\n",
        "\n",
        "        # Relative strength of volume\n",
        "        # Compare current volume to recent history\n",
        "        for window in [5, 20]:\n",
        "            vol_ma = data['Volume'].rolling(window=window).mean()\n",
        "            features[f'volume_strength_{window}d'] = data['Volume'] / vol_ma\n",
        "\n",
        "        # Bid-ask spread proxy using high-low range\n",
        "        # Since we might not have actual bid-ask data\n",
        "        features['hl_spread'] = (data['High'] - data['Low']) / typical_price\n",
        "\n",
        "        # Volume volatility\n",
        "        # High volume volatility can indicate unstable liquidity conditions\n",
        "        for window in [5, 20]:\n",
        "            features[f'volume_volatility_{window}d'] = (\n",
        "                data['Volume']\n",
        "                .rolling(window=window)\n",
        "                .std() / data['Volume'].rolling(window=window).mean()\n",
        "            )\n",
        "\n",
        "        # Flow toxicity measure\n",
        "        # Adapted from VPIN (Volume-synchronized Probability of Informed Trading)\n",
        "        # High values might indicate informed trading and potential liquidity problems\n",
        "        signed_volume = data['Volume'] * np.sign(data['Close'] - data['Open'])\n",
        "        features['flow_toxicity'] = (\n",
        "            signed_volume\n",
        "            .rolling(window=20)\n",
        "            .sum()\n",
        "            .abs() / data['Volume'].rolling(window=20).sum()\n",
        "        )\n",
        "\n",
        "        # Liquidity resilience\n",
        "        # How quickly volume returns after a high-volume day\n",
        "        high_volume_days = data['Volume'] > data['Volume'].rolling(window=20).mean() * 1.5\n",
        "        volume_after_shock = (\n",
        "            data['Volume']\n",
        "            .shift(-1)  # Next day's volume\n",
        "            .rolling(window=5)\n",
        "            .mean()\n",
        "        )\n",
        "        features['liquidity_resilience'] = volume_after_shock / data['Volume']\n",
        "\n",
        "        # Market efficiency coefficient\n",
        "        # Helps identify if price changes are orderly or disorderly\n",
        "        for window in [5, 20]:\n",
        "            log_prices = np.log(data['Close'])\n",
        "            variance_ratio = (\n",
        "                log_prices.diff(window).var()\n",
        "                / (window * log_prices.diff().var())\n",
        "            )\n",
        "            features[f'market_efficiency_{window}d'] = variance_ratio\n",
        "\n",
        "        # Handle any NaN values from the calculations\n",
        "        features = features.fillna(method='bfill')\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "    def prepare_training_data(self, sequence_length: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Prepares features and labels for training\n",
        "        \"\"\"\n",
        "        # Calculate all features\n",
        "        features = self._calculate_features(self.data)\n",
        "\n",
        "        # Create labels (future returns)\n",
        "        labels = self._create_labels(self.data)\n",
        "\n",
        "        # Convert to tensors and create sequences\n",
        "        X, y = self._create_sequences(features, labels, sequence_length)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def _create_sequences(self,\n",
        "                         features: pd.DataFrame,\n",
        "                         labels: pd.DataFrame,\n",
        "                         sequence_length: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Creates sequences for training\n",
        "        \"\"\"\n",
        "        X, y = [], []\n",
        "\n",
        "        for i in range(len(features) - sequence_length):\n",
        "            X.append(features.iloc[i:i+sequence_length].values)\n",
        "            y.append(labels.iloc[i+sequence_length])\n",
        "\n",
        "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
        "\n",
        "    def _create_labels(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Creates labels for training\n",
        "        \"\"\"\n",
        "        labels = pd.DataFrame(index=data.index)\n",
        "\n",
        "        # Future returns\n",
        "        future_returns = data['Close'].pct_change().shift(-1)\n",
        "\n",
        "        # Create binary direction label\n",
        "        labels['direction'] = (future_returns > 0).astype(float)\n",
        "\n",
        "        # Create magnitude label\n",
        "        labels['magnitude'] = future_returns.abs()\n",
        "\n",
        "        return labels"
      ],
      "metadata": {
        "id": "CmbJj2vRBzE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model with basic parameters\n",
        "model = HybridFinancialModel(\n",
        "    state_dim=32,      # Size of continuous state\n",
        "    memory_dim=64,     # Size of attention/memory state\n",
        "    input_dim=20,      # Number of input features\n",
        "    num_heads=4        # Start with fewer attention heads\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = ModelTrainer(\n",
        "    model=model,\n",
        "    training_start='2010-01-01',\n",
        "    training_end='2022-12-31'\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train_model(\n",
        "    epochs=100,\n",
        "    learning_rate=1e-4\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "oGjsX75XB1dJ",
        "outputId": "c182c456-d366-4329-faec-d729c3e3a4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ModelTrainer' object has no attribute 'train_data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-edf1360470c9>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m trainer.train_model(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-2ccaef27d2e8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Create data loaders using our stored data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         train_loader = self.create_data_loader(\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ModelTrainer' object has no attribute 'train_data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create validation environment\n",
        "paper_trading = PaperTradingEnvironment(\n",
        "    initial_capital=1_000_000,\n",
        "    transaction_costs=True\n",
        ")\n",
        "\n",
        "# Run validation\n",
        "validation_results = paper_trading.run_validation(\n",
        "    model=model,\n",
        "    start_date='2023-01-01',\n",
        "    end_date='2023-12-31'\n",
        ")\n",
        "\n",
        "# Analyze results\n",
        "analysis = StrategyAnalyzer(validation_results)\n",
        "analysis.print_summary()"
      ],
      "metadata": {
        "id": "nSGKq5QAB7A4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}